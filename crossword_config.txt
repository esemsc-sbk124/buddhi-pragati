# ==========================================
#           BUDDHI-PRAGATI 
#       DATASET CONFIGURATION
# ==========================================
# 
# This file contains all tunable parameters for dataset creation.
# Modify values here to experiment without changing code.
# 
# Format: PARAMETER_NAME=value
# Lines starting with # are comments and ignored
# 

# ==========================================
#        DATASET CREATION
# ==========================================

# Controls how many raw entries are loaded from each source at once
# Used in: buddhi_pragati/data/dataset_builder.py lines 253, 458
# Impact: Larger values = more memory usage but fewer API calls
BATCH_SIZE_PROCESSING=400

# Default number of entries to generate per language when not specified
# Used in: buddhi_pragati/data/dataset_builder.py line 370
# Impact: Controls dataset size for crossword generation corpus
TARGET_DATASET_SIZE_PER_LANGUAGE=1000

# Minimum quality threshold for entries to be included in dataset
# Used in: buddhi_pragati/data/dataset_builder.py lines 255, 569
# Impact: Higher values = better quality but fewer entries
MIN_QUALITY_SCORE=0.3

# NOTE: Memory optimization parameters MAX_ENTRIES_PER_BATCH and MEMORY_BATCH_LIMIT 
# have been removed as the system now uses true incremental upload logic:
# - Upload mode: Each batch is uploaded immediately after processing (no accumulation)
# - Accumulation mode: Only used for redistribution, memory bounded by BATCH_SIZE_PROCESSING

# HuggingFace repository ID for dataset uploads
# Used in: buddhi_pragati/data/dataset_builder.py lines 487, 582
# Impact: Determines where datasets are stored on HuggingFace Hub
HF_DATASET_REPO=selim-b-kh/buddhi-pragati

# Default data sources when none specified in CLI
# Used in: buddhi_pragati/data/dataset_builder.py line 374
# Impact: Controls which datasets are used for crossword clue generation
DEFAULT_DATASET_SOURCES=MILU,IndicWikiBio,IndoWordNet,Bhasha-Wiki

# ==========================================
#         WORD FILTERING
# ==========================================

# Word length constraints
MIN_WORD_LENGTH=2
MAX_WORD_LENGTH=12

# Clue length constraints
MIN_CLUE_LENGTH=10
MAX_CLUE_LENGTH=500

# ==========================================
#      SOURCE-SPECIFIC PARAMETERS
# ==========================================

# MILU dataset filtering
MILU_FILTER_CONTEXTUAL_QUESTIONS=true

# Bhasha-Wiki processing
BHASHA_WIKI_MIN_TEXT_LENGTH=50
NER_MODEL_NAME=ai4bharat/IndicNER
MAX_ENTITIES_PER_TEXT=5
REQUIRE_SINGLE_WORD_ENTITIES=true

# IndicWikiBio processing
INDIC_WIKIBIO_MIN_SUMMARY_LENGTH=30

# IndoWordNet processing
INDOWORDNET_MIN_DEFINITION_LENGTH=5

# ==========================================
#         CLI DEFAULTS
# ==========================================

# Default API credentials (leave empty to force environment lookup)
DEFAULT_HF_TOKEN=

# ==========================================
#       ENHANCED CONTEXT SCORING
# ==========================================

# Primary multilingual model for universal language coverage (all 20 languages)
# Used for semantic similarity against Indian cultural corpus
# Used in: buddhi_pragati/utils/indian_context_scorer.py
INDIAN_CONTEXT_PRIMARY_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Indic-specific model for enhanced accuracy on core 11 languages  
# Used for dual-model scoring on Bengali, English, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu
# Used in: buddhi_pragati/utils/indian_context_scorer.py
INDIAN_CONTEXT_INDIC_MODEL=l3cube-pune/indic-sentence-similarity-sbert

# Enable tiered scoring strategy (true/false)
# When true: Use dual models for tier 2 languages, single model for tier 1
# When false: Use only primary model for all languages
ENABLE_TIERED_CONTEXT_SCORING=true

# Scoring weights for Tier 1 languages (9 languages with limited model support)
# Format: embedding_weight,keyword_weight (must sum to 1.0)
# embedding: Semantic similarity using multilingual model
# keyword: Cultural keyword matching score
CONTEXT_SCORING_WEIGHTS_TIER1=0.85,0.15

# Scoring weights for Tier 2 languages (11 languages with enhanced model support)  
# Format: multilingual_weight,indic_weight,keyword_weight (must sum to 1.0)
# multilingual: Semantic similarity using primary multilingual model
# indic: Semantic similarity using Indic-specific model
# keyword: Cultural keyword matching score
CONTEXT_SCORING_WEIGHTS_TIER2=0.85,0.0,0.15

# ==========================================
#     CROSSWORD PUZZLE GENERATION  
# ==========================================

# Grid density targets (0.0-1.0)
TARGET_GRID_DENSITY=0.75
MIN_ACCEPTABLE_DENSITY=0.65

# Generation control
MAX_GENERATION_ATTEMPTS=50
MIN_WORDS_PER_PUZZLE=5
GENERATION_TIMEOUT_SECONDS=30

# Cultural context classification  
INDIAN_CONTEXT_THRESHOLD=0.5
PREFER_INDIAN_ENTRIES=true

# Quality scoring weights (must sum to 1.0)
DENSITY_WEIGHT=0.6
INTERSECTION_WEIGHT=0.25
CULTURAL_COHERENCE_WEIGHT=0.15

# Grid size optimization (wide range support)
MIN_GRID_SIZE=3
MAX_GRID_SIZE=30
PREFERRED_WORD_LENGTH_RANGE=3,10
DEFAULT_GRID_SIZE=15

# Memory and batch processing
CROSSWORD_BATCH_SIZE=10
MAX_CONCURRENT_ATTEMPTS=5
CLEANUP_FAILED_ATTEMPTS=true
RETRY_WITH_SMALLER_CORPUS=true

# Generation defaults
DEFAULT_PUZZLE_COUNT=10

# Generated puzzle dataset repository
HF_GENERATED_PUZZLES_REPO=selim-b-kh/buddhi-pragati-puzzles

# ==========================================
#         EVALUATION CONFIGURATION
# ==========================================

# Model Selection and Experimental Defaults
DEFAULT_EVALUATION_MODELS=gpt-4o,claude-sonnet-4
DEFAULT_EXPERIMENT_TYPES=shot-variations,chain-of-thought
DEFAULT_REASONING_EFFORT=normal
DEFAULT_EVALUATION_BATCH_SIZE=1

# Priority Settings for Focused Experiments (2-7)
# These are used to limit scope for experiments while maintaining thoroughness
DEFAULT_PRIORITARY_GRID_SIZES=7,15,25
DEFAULT_PRIORITARY_LANGUAGES=Bengali,English,Gujarati,Hindi,Kannada,Malayalam,Odia,Tamil,Telugu,Urdu
# Removed OpenRouter models due to rate limits: deepseek, google/gemini, mistralai, moonshotai, qwen
DEFAULT_PRIORITARY_MODELS=gpt-4o,claude-sonnet-4,sarvamai/sarvam-m,ai4bharat/Airavata,gpt-5,o3,claude-opus-4-1-20250805,CohereForAI/aya-101,Cognitive-Lab/Ambari-7B-Instruct-v0.1,nickmalhotra/ProjectIndus,smallstepai/Misal-7B-instruct-v0.1,abhinand/tamil-llama-7b-instruct-v0.2

# Few-shot Learning Configuration
FEW_SHOT_COUNT=3
DEFAULT_SHOT_TYPE=zero-shot
SELF_REFLECTION_MAX_ITERATIONS=3

# Token Limits for Reasoning Efforts (Research-Based)
# Based on academic literature and model documentation
LOW_REASONING_TOKENS=500
NORMAL_REASONING_TOKENS=1000
HIGH_REASONING_TOKENS=2000

# Prompt Configuration Defaults
DEFAULT_CHAIN_OF_THOUGHT=false
DEFAULT_ENABLE_CHAIN_OF_THOUGHT=false
DEFAULT_SELF_REFLECTION=false
DEFAULT_ENABLE_SELF_REFLECTION=false
DEFAULT_REASONING_EFFORT_LEVEL=normal
DEFAULT_BATCH_SIZE_EVALUATION=1

# Performance Tracking and Analysis
ENABLE_COST_TRACKING=true
ENABLE_TOKEN_TRACKING=true
EXPERIMENT_RESULTS_DIR=buddhi_pragati/experiments

# Experiment Execution Configuration
MAX_PUZZLES_PER_FOCUSED_EXPERIMENT=20
MAX_CONCURRENT_EXPERIMENTS=1
AUTO_GENERATE_REPORTS=true
ENABLE_PROGRESS_TRACKING=true

# Grid Size Configuration for Experiments
# Used by master experiment (0) for comprehensive coverage
ALL_GRID_SIZES=7,10,15,20,25
MIN_GRID_SIZE=7
MAX_GRID_SIZE=25

# Language Coverage for Experiments
# Priority languages are used for focused experiments (2-7)
# All languages are used for master experiment (0)
ALL_SUPPORTED_LANGUAGES=Assamese,Bengali,Bodo,English,Gujarati,Hindi,Kannada,Kashmiri,Konkani,Malayalam,Marathi,Meitei,Nepali,Odia,Punjabi,Sanskrit,Tamil,Telugu,Urdu

# Results Analysis Configuration
# Generate statistical summaries in reports
# Compare results across different experiments
# P-value threshold for statistical tests
# Default format for generated plots (png, svg, pdf)
ENABLE_STATISTICAL_ANALYSIS=true
ENABLE_CROSS_EXPERIMENT_COMPARISON=true
DEFAULT_SIGNIFICANCE_LEVEL=0.05
VISUALIZATION_FORMAT=png

# ==========================================
#         API KEYS AND CREDENTIALS
# ==========================================

# Default API credentials for all providers
# Leave empty to force environment variable lookup
DEFAULT_OPENAI_API_KEY=
DEFAULT_OPENROUTER_API_KEY=

# Old API KEYS
OLD_SARVAM_API_KEY=sk_wwy052yk_IgV2rhe8KqCAxwipvUMmf4yt


# ==========================================
#         MODEL INTERFACE DEFAULTS
# ==========================================

# Default model source and model for evaluation
DEFAULT_MODEL=gpt-4o
DEFAULT_MODEL_SOURCE=openai
DEFAULT_LANGUAGE=English

# Temperature settings for different model types
# Reasoning models often work better with 0 temperature
# Standard models with low but non-zero temperature
# Standard max tokens for non-reasoning models
DEFAULT_TEMPERATURE_REASONING=0.0
DEFAULT_TEMPERATURE_STANDARD=0.1
DEFAULT_MAX_TOKENS=1000

# ==========================================
#         PROVIDER TIMEOUT SETTINGS
# ==========================================

# Timeout settings per provider (in seconds)
# Longer timeouts for providers that may have slower response times
OPENAI_TIMEOUT_SECONDS=120
ANTHROPIC_TIMEOUT_SECONDS=120
SARVAM_TIMEOUT_SECONDS=60
OPENROUTER_TIMEOUT_SECONDS=180
HUGGINGFACE_TIMEOUT_SECONDS=300

# ==========================================
#     HUGGINGFACE MEMORY MANAGEMENT
# ==========================================

# Memory management for HuggingFace models
# Maximum memory allocation in MB
HF_MAX_MEMORY_MB=8192
# Enable low memory mode for CPU inference
HF_LOW_MEMORY_MODE=true
# Device mapping strategy (auto, sequential, balanced)
HF_DEVICE_MAP_STRATEGY=auto
# Maximum sequence length for truncation
HF_MAX_SEQUENCE_LENGTH=2048

# ==========================================
#         RETRY CONFIGURATION
# ==========================================

# Retry settings for API failures
# Maximum number of retry attempts
MAX_RETRY_ATTEMPTS=3
# Initial delay between retries (seconds)
INITIAL_RETRY_DELAY=1.0
# Maximum delay between retries (seconds)
MAX_RETRY_DELAY=60.0
# Exponential backoff factor
BACKOFF_FACTOR=2.0
# Enable automatic retry on rate limits
RETRY_ON_RATE_LIMIT=true     